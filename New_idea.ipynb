{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "###config에서 바로 성능 예측 (tps, latency by MLP)\n",
    "import os\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import configparser\n",
    "os.chdir(\"/home/sein/ksc_model/model\")\n",
    "\n",
    "def mysql_knob_dataframe(wk, knobs_path):\n",
    "    knobs_path = os.path.join(knobs_path, str(wk))\n",
    "    config_len = len(os.listdir(knobs_path))\n",
    "    cnf_parser = configparser.ConfigParser()\n",
    "    pd_mysql = pd.DataFrame()\n",
    "    for idx in range(config_len):\n",
    "        cnf_parser.read(os.path.join(knobs_path, f'my_{idx}.cnf'))\n",
    "        conf_dict = cnf_parser._sections['mysqld']\n",
    "        tmp = pd.DataFrame(data=[conf_dict.values()], columns=conf_dict.keys())\n",
    "        pd_mysql = pd.concat([pd_mysql, tmp])\n",
    "        \n",
    "    pd_mysql = pd_mysql.reset_index(drop=True)\n",
    "    pd_mysql = pd_mysql.drop(columns=['log-error', 'bind-address'])\n",
    "    return pd_mysql\n",
    "\n",
    "def get_data(knob_path, external_path, wk):\n",
    "    raw_knobs = mysql_knob_dataframe(wk, knob_path)\n",
    "    external_ = pd.read_csv(os.path.join(external_path, f'external_results_{wk}.csv'), index_col=0)\n",
    "    latency_columns = []\n",
    "    for col in external_.columns:\n",
    "        if col.find(\"latency\") == 0 and col != 'latency_max' and col != 'latency_CLEANUP':\n",
    "            latency_columns.append(col)\n",
    "    external = external_[['tps']].copy()\n",
    "    external['latency'] = external_[latency_columns].max(axis=1)\n",
    "    return raw_knobs, external\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/sein/ksc_model/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Pred_Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Pred_Model, self).__init__()\n",
    "        # self.knob_fc = nn.Sequential(nn.Linear(self.input_dim, self.hidden_dim), nn.ReLU())  #hidden layer\n",
    "        # self.im_fc = nn.Sequential(nn.Linear(self.hidden_dim, self.output_dim), nn.Sigmoid()) #output layer\n",
    "    # dropout = nn.Dropout(p=0.25)\n",
    "        #--------------------------------------\n",
    "        self.input_dim = input_dim # 22\n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.hidden_dim2 = hidden_dim//2\n",
    "        # self.hidden_dim3 = hidden_dim//4\n",
    "        # self.hidden_dim4 = hidden_dim//2\n",
    "        self.output_dim = output_dim #3\n",
    "       #--------------------------------------\n",
    "       \n",
    "       \n",
    "        # self.weight = nn.parameter (torch.FloatTensor(7,32,32, device=\"cuda\"))\n",
    "        \n",
    "        \n",
    "        self.Layer1 = nn.Sequential(nn.Linear(self.input_dim, self.hidden_dim), nn.GELU())  #hidden layer\n",
    "        self.Layer2 = nn.Sequential(nn.Linear(self.hidden_dim, self.hidden_dim2), nn.GELU())\n",
    "        # self.Layer3 = nn.Sequential(nn.Linear(self.hidden_dim2, self.hidden_dim3), nn.ReLU())\n",
    "        # self.Layer4 = nn.Sequential(nn.Linear(self.hidden_dim3, self.hidden_dim4), nn.ReLU())\n",
    "        self.Layer3 = nn.Sequential(nn.Linear(self.hidden_dim2, self.output_dim))\n",
    "        \n",
    "        #self.Layer4 = nn.Sequential(nn.LayerNorm(self.hidden_dim3),nn.Linear(self.hidden_dim3, self.output_dim), nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = self.Layer1(x)\n",
    "        x = self.Layer2(x)\n",
    "        x = self.Layer3(x)\n",
    "        # x = self.Layer4(x)\n",
    "        # x = self.Layer5(x)\n",
    "\n",
    "        # self.x_kb = self.knob_fc(x)\n",
    "        # self.\n",
    "        # self.x_im = self.im_fc(self.x_kb)\n",
    "\n",
    "      \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1epoch] tr_loss:0.2855 | val_loss:0.2815 | R2_Score:[-47.28687282  -0.16777562]\n",
      "[11epoch] tr_loss:0.1832 | val_loss:0.1813 | R2_Score:[-29.66356738  -0.04159952]\n",
      "[21epoch] tr_loss:0.0934 | val_loss:0.0935 | R2_Score:[-14.2158887    0.05283572]\n",
      "[31epoch] tr_loss:0.0409 | val_loss:0.0410 | R2_Score:[-5.01806931  0.13249039]\n",
      "[41epoch] tr_loss:0.0230 | val_loss:0.0236 | R2_Score:[-2.01794002  0.21392161]\n",
      "[51epoch] tr_loss:0.0180 | val_loss:0.0185 | R2_Score:[-1.21516912  0.28311025]\n",
      "[61epoch] tr_loss:0.0158 | val_loss:0.0164 | R2_Score:[-0.92964998  0.33457579]\n",
      "[71epoch] tr_loss:0.0152 | val_loss:0.0151 | R2_Score:[-0.76442737  0.37426559]\n",
      "[81epoch] tr_loss:0.0139 | val_loss:0.0141 | R2_Score:[-0.63396984  0.40552327]\n",
      "[91epoch] tr_loss:0.0129 | val_loss:0.0133 | R2_Score:[-0.52357493  0.43312833]\n",
      "[101epoch] tr_loss:0.0118 | val_loss:0.0125 | R2_Score:[-0.4185635   0.45785093]\n",
      "[111epoch] tr_loss:0.0112 | val_loss:0.0117 | R2_Score:[-0.32219519  0.48057582]\n",
      "[121epoch] tr_loss:0.0106 | val_loss:0.0110 | R2_Score:[-0.23202322  0.50297703]\n",
      "[131epoch] tr_loss:0.0102 | val_loss:0.0104 | R2_Score:[-0.14799333  0.52394916]\n",
      "[141epoch] tr_loss:0.0092 | val_loss:0.0098 | R2_Score:[-0.07181476  0.54530564]\n",
      "[151epoch] tr_loss:0.0088 | val_loss:0.0092 | R2_Score:[-2.02919193e-04  5.65200320e-01]\n",
      "[161epoch] tr_loss:0.0082 | val_loss:0.0087 | R2_Score:[0.06697467 0.58390515]\n",
      "[171epoch] tr_loss:0.0078 | val_loss:0.0082 | R2_Score:[0.12949522 0.60305751]\n",
      "[181epoch] tr_loss:0.0082 | val_loss:0.0077 | R2_Score:[0.18808141 0.6221239 ]\n",
      "[191epoch] tr_loss:0.0068 | val_loss:0.0072 | R2_Score:[0.24061748 0.6389224 ]\n",
      "[201epoch] tr_loss:0.0074 | val_loss:0.0068 | R2_Score:[0.29051266 0.65568603]\n",
      "[211epoch] tr_loss:0.0060 | val_loss:0.0064 | R2_Score:[0.33404896 0.67082789]\n",
      "[221epoch] tr_loss:0.0056 | val_loss:0.0061 | R2_Score:[0.37627903 0.68429048]\n",
      "[231epoch] tr_loss:0.0053 | val_loss:0.0057 | R2_Score:[0.41536426 0.69845777]\n",
      "[241epoch] tr_loss:0.0051 | val_loss:0.0054 | R2_Score:[0.4514752  0.71114503]\n",
      "[251epoch] tr_loss:0.0048 | val_loss:0.0052 | R2_Score:[0.48440241 0.72200836]\n",
      "[261epoch] tr_loss:0.0047 | val_loss:0.0049 | R2_Score:[0.51364266 0.73221524]\n",
      "[271epoch] tr_loss:0.0044 | val_loss:0.0047 | R2_Score:[0.54047143 0.74080788]\n",
      "[281epoch] tr_loss:0.0042 | val_loss:0.0045 | R2_Score:[0.56710746 0.74966016]\n",
      "[291epoch] tr_loss:0.0042 | val_loss:0.0043 | R2_Score:[0.5894616  0.75658297]\n",
      "[301epoch] tr_loss:0.0042 | val_loss:0.0041 | R2_Score:[0.61066783 0.76310618]\n",
      "[311epoch] tr_loss:0.0037 | val_loss:0.0040 | R2_Score:[0.63073016 0.76872392]\n",
      "[321epoch] tr_loss:0.0037 | val_loss:0.0038 | R2_Score:[0.64908775 0.77395871]\n",
      "[331epoch] tr_loss:0.0035 | val_loss:0.0037 | R2_Score:[0.6647088  0.77879543]\n",
      "[341epoch] tr_loss:0.0034 | val_loss:0.0036 | R2_Score:[0.6804249  0.78282526]\n",
      "[351epoch] tr_loss:0.0033 | val_loss:0.0035 | R2_Score:[0.69467907 0.78617724]\n",
      "[361epoch] tr_loss:0.0032 | val_loss:0.0034 | R2_Score:[0.7074882  0.78907487]\n",
      "[371epoch] tr_loss:0.0032 | val_loss:0.0033 | R2_Score:[0.71929972 0.79131777]\n",
      "[381epoch] tr_loss:0.0031 | val_loss:0.0032 | R2_Score:[0.73013922 0.79414528]\n",
      "[391epoch] tr_loss:0.0031 | val_loss:0.0031 | R2_Score:[0.74030071 0.79624024]\n",
      "[401epoch] tr_loss:0.0030 | val_loss:0.0031 | R2_Score:[0.74945996 0.79804487]\n",
      "[411epoch] tr_loss:0.0029 | val_loss:0.0030 | R2_Score:[0.75813991 0.79982179]\n",
      "[421epoch] tr_loss:0.0029 | val_loss:0.0030 | R2_Score:[0.76546579 0.8012271 ]\n",
      "[431epoch] tr_loss:0.0028 | val_loss:0.0029 | R2_Score:[0.77419739 0.80271026]\n",
      "[441epoch] tr_loss:0.0029 | val_loss:0.0029 | R2_Score:[0.78027668 0.80388605]\n",
      "[451epoch] tr_loss:0.0028 | val_loss:0.0028 | R2_Score:[0.78476423 0.80508627]\n",
      "[461epoch] tr_loss:0.0028 | val_loss:0.0028 | R2_Score:[0.79037849 0.8055844 ]\n",
      "[471epoch] tr_loss:0.0027 | val_loss:0.0028 | R2_Score:[0.79509055 0.80635216]\n",
      "[481epoch] tr_loss:0.0027 | val_loss:0.0027 | R2_Score:[0.79986323 0.80721657]\n",
      "[491epoch] tr_loss:0.0027 | val_loss:0.0027 | R2_Score:[0.80453701 0.80844171]\n",
      "[501epoch] tr_loss:0.0027 | val_loss:0.0027 | R2_Score:[0.80709544 0.80844022]\n",
      "[511epoch] tr_loss:0.0027 | val_loss:0.0027 | R2_Score:[0.80972759 0.8086106 ]\n",
      "[521epoch] tr_loss:0.0027 | val_loss:0.0026 | R2_Score:[0.81348615 0.80910899]\n",
      "[531epoch] tr_loss:0.0026 | val_loss:0.0026 | R2_Score:[0.81603361 0.80975194]\n",
      "[541epoch] tr_loss:0.0027 | val_loss:0.0026 | R2_Score:[0.81865517 0.81010385]\n",
      "[551epoch] tr_loss:0.0026 | val_loss:0.0026 | R2_Score:[0.82137284 0.81106857]\n",
      "[561epoch] tr_loss:0.0027 | val_loss:0.0026 | R2_Score:[0.82257545 0.81063231]\n",
      "[571epoch] tr_loss:0.0026 | val_loss:0.0026 | R2_Score:[0.82436029 0.81120082]\n",
      "[581epoch] tr_loss:0.0026 | val_loss:0.0026 | R2_Score:[0.82558584 0.8111344 ]\n",
      "[591epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.82818542 0.81201827]\n",
      "[601epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.82986299 0.81222995]\n",
      "[611epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83057702 0.81217741]\n",
      "[621epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.8318339  0.81240227]\n",
      "[631epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83188418 0.8129044 ]\n",
      "[641epoch] tr_loss:0.0025 | val_loss:0.0025 | R2_Score:[0.83366453 0.81287287]\n",
      "[651epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83420812 0.81281596]\n",
      "[661epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.8345392  0.81293228]\n",
      "[671epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83518924 0.81311746]\n",
      "[681epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83566664 0.81308157]\n",
      "[691epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83669071 0.81312431]\n",
      "[701epoch] tr_loss:0.0037 | val_loss:0.0025 | R2_Score:[0.83677537 0.8130559 ]\n",
      "[711epoch] tr_loss:0.0037 | val_loss:0.0025 | R2_Score:[0.83699173 0.81345801]\n",
      "[721epoch] tr_loss:0.0036 | val_loss:0.0025 | R2_Score:[0.83741001 0.81357061]\n",
      "[731epoch] tr_loss:0.0027 | val_loss:0.0025 | R2_Score:[0.83703408 0.81316724]\n",
      "[741epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83817935 0.81351705]\n",
      "[751epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83610331 0.81314616]\n",
      "[761epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83774605 0.81371829]\n",
      "[771epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83815613 0.8138726 ]\n",
      "[781epoch] tr_loss:0.0030 | val_loss:0.0025 | R2_Score:[0.83980477 0.81395011]\n",
      "[791epoch] tr_loss:0.0026 | val_loss:0.0025 | R2_Score:[0.83817681 0.81304819]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### conf - performance prediction model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "random_seed = 777\n",
    "import random\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "#workload6에 대한 external metric\n",
    "KNOB_PATH = ('../data/configs_new_dataset/')\n",
    "EXTERNAL_PATH = (\"../data/configs_new_dataset/external_new_dataset/\")\n",
    "wk = 6\n",
    "\n",
    "# print(EXTERNAL)\n",
    "\n",
    "raw_knobs, external = get_data(KNOB_PATH, EXTERNAL_PATH, wk)\n",
    "# print(raw_knobs)\n",
    "config_Preprocessing = []\n",
    "raw_knobsT = raw_knobs.transpose()\n",
    "# print(raw_knobsT) \n",
    "for i in range(20):\n",
    "    chunk_config = raw_knobsT.iloc[i]\n",
    "    # print(chunk_config)\n",
    "    chunk_config = chunk_config.values.reshape(-1, 1) # (2000,1)\n",
    "    # print(chunk_config)\n",
    "    # chunk_config = raw_knobs.iloc[:,[i]]\n",
    "    scaler_config = StandardScaler().fit(chunk_config)\n",
    "    # print(scaler_config)\n",
    "    Preprocessing = scaler_config.transform(chunk_config)\n",
    "    Preprocessing = pd.DataFrame(Preprocessing) # (2000,1)\n",
    "    # print(Preprocessing)\n",
    "    config_Preprocessing.append(Preprocessing)\n",
    "    # print(config_Preprocessing)\n",
    "config_Preprocessing = pd.concat(config_Preprocessing, axis=1)\n",
    "#print(config_Preprocessing.shape)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     config_Preprocessing.append(Preprocessing)\n",
    "#     config_Preprocessing = pd.DataFrame(config_Preprocessing)\n",
    "#     #여기까지 \n",
    "# print(config_Preprocessing)\n",
    "# config_Preprocessing = pd.concat([Preprocessing], axis = 1)\n",
    "    \n",
    "\n",
    "    # config_Preprocessing \n",
    "    # print(config_Preprocessing)  \n",
    "# config_Preprocessing = pd.concat([Preprocessing], axis = 1)\n",
    "      \n",
    "# config_Preprocessing.append(Preprocessing)\n",
    "# print(config_Preprocessing)\n",
    "\n",
    "    # config_Preprocessing = pd.concat([Preprocessing], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# print(external)\n",
    "#raw_knobs 대한 scaling\n",
    "# scaler_config = MinMaxScaler().fit(raw_knobs)\n",
    "# config_Preprocessing = scaler_config.transform(raw_knobs)\n",
    "# config_Preprocessing = pd.DataFrame(config_Preprocessing)\n",
    "\n",
    "\n",
    "# scaler_metrics = Normalizer().fit(external)\n",
    "# metrics_Preprocessing = scaler_metrics.transform(external)\n",
    "# metrics_Preprocessing = pd.DataFrame(metrics_Preprocessing)\n",
    "\n",
    "tps = external['tps']\n",
    "latency = external['latency']\n",
    "tps = tps.values.reshape(-1,1)\n",
    "latency = latency.values.reshape(-1,1)\n",
    "scaled_tps = MinMaxScaler().fit(tps)\n",
    "# scaled_tps = scaled_tps.reshape(-1, 1)\n",
    "tps_preprocessing = scaled_tps.transform(tps)\n",
    "tps_preprocessing = pd.DataFrame(tps_preprocessing)\n",
    "\n",
    "scaled_latency = MinMaxScaler().fit(latency)\n",
    "# scaled_latency = scaled_latency.reshape(-1, 1)\n",
    "latency_preprocessing = scaled_tps.transform(latency)\n",
    "latency_preprocessing = pd.DataFrame(latency_preprocessing)\n",
    "\n",
    "external_preprocessing = pd.concat([tps_preprocessing, latency_preprocessing], axis =1)\n",
    "\n",
    "\n",
    "\n",
    "# scaled_external = StandardScaler().fit(external)\n",
    "# external_preprocessing = scaled_external.transform(external)\n",
    "# external_preprocessing = pd.DataFrame(external_preprocessing)\n",
    "\n",
    "# np_external = external.to_numpy()\n",
    "# scaler_metrics = MinMaxScaler().fit(np_external)\n",
    "# metrics_Preprocessing = scaler_metrics.transform(np_external)\n",
    "# metrics_Preprocessing = pd.DataFrame(metrics_Preprocessing)\n",
    "# metrics_Preprocessing = metrics_Preprocessing.astype('float')\n",
    "# print(type(config_Preprocessing))\n",
    "# print(type(metrics_Preprocessing))\n",
    "#X = configuraion, Y = scaling external metrics\n",
    "X_train, X_test, y_train, y_test = train_test_split(config_Preprocessing, external_preprocessing, test_size=0.4, shuffle=True)\n",
    "\n",
    "\n",
    "# train_index = X_train.index\n",
    "# test_index = X_test.index\n",
    "\n",
    "#X,Y값 scaling\n",
    "# X_scaler = StandardScaler().fit(X_train)\n",
    "# Y_scaler = StandardScaler().fit(y_train)\n",
    "\n",
    "scaled_X_train = X_train\n",
    "scaled_X_test = X_test\n",
    "scaled_Y_train = y_train\n",
    "scaled_Y_test = y_test\n",
    "\n",
    "\n",
    "\n",
    "# scaled_X_train = X_scaler.transform(X_train)\n",
    "# scaled_X_test = X_scaler.transform(X_test)\n",
    "# scaled_Y_train = Y_scaler.transform(y_train)\n",
    "# scaled_Y_test = Y_scaler.transform(y_test)\n",
    "# train_TPS = y_train[:,0]\n",
    "# train_Latency = y_train[:,1]\n",
    "# test_TPS = y_test[:,0]\n",
    "# test_Latency = y_test[:,1]\n",
    "import torch\n",
    "# scaled_X_train, scaled_Y_train = scaled_X_train.values, scaled_Y_train.values\n",
    "# scaled_X_test, scaled_Y_test = scaled_X_test.values, scaled_Y_test.values\n",
    "scaled_X_train, scaled_Y_train = torch.Tensor(scaled_X_train.values), torch.Tensor(scaled_Y_train.values).cuda()\n",
    "scaled_X_test, scaled_Y_test = torch.Tensor(scaled_X_test.values), torch.Tensor(scaled_Y_test.values).cuda()\n",
    "\n",
    "\n",
    "# print(scaled_X_train.type())\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataset_tr = TensorDataset(scaled_X_train, scaled_Y_train)\n",
    "dataset_te = TensorDataset(scaled_X_test, scaled_Y_test)\n",
    "# dataset_tr = (scaled_X_train, scaled_Y_train)\n",
    "# dataset_te = (scaled_X_test, scaled_Y_test)\n",
    "## prepare experiment\n",
    "batch_size = 128\n",
    "dataloader_tr = DataLoader(dataset_tr, batch_size=batch_size, shuffle=True)\n",
    "dataloader_te = DataLoader(dataset_te, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "#Attention\n",
    "# network = model(input_dim = len(scaled_X_train[0]), hidden_dim = 32, output_dim = 2).cuda()\n",
    "# network = ReshapeNet(input_dim = len(scaled_X_train[0]), hidden_dim = 32, output_dim = 2, group_dim=8).cuda()\n",
    "criterion = nn.MSELoss().cuda() #regression\n",
    "# loss = F.mse_loss(output, target)\n",
    "####추후 수정\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "network = Pred_Model(input_dim = len(scaled_X_train[0]), hidden_dim = 64, output_dim = 2).cuda()\n",
    "    #train\n",
    "# optimizer = optim.Adam(network.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# def get_lr(optimizer):\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         return param_group['lr']\n",
    "\n",
    "for epoch in range(800):\n",
    "    #loss_tr = self.train(self.model, dataloader_tr)\n",
    "    total_loss = 0.\n",
    "    # total_acc = 0.\n",
    "    ## start iteration\n",
    "    network.train()\n",
    "    # for parameter in network.parameters():\n",
    "    #     print(parameter.requires_grad) \n",
    "    for data, target in dataloader_tr:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data = np.array(data)\n",
    "        # onehot = clf.predict(data) #one-hot classification model의 결과\n",
    "        \n",
    "        # stack_onehot = []\n",
    "        \n",
    "        # for i in onehot:\n",
    "        #    onehot_li = [0,0,0]\n",
    "        #    onehot_li[i] += 1 \n",
    "        #    stack_onehot.append(onehot_li)\n",
    "        # stack_onehot = torch.tensor(stack_onehot)\n",
    "       \n",
    "        # #stack_onehot = torch.stack(torch.tensor(stack_onehot))\n",
    "        # #print(stack_onehot.shape)\n",
    "        # cluster = stack_onehot.float().cuda()\n",
    "        \n",
    "        data = torch.tensor(data).float().cuda()\n",
    "        \n",
    "        #x,y 를 받음 / conf-linear-attention & onehot-linear-attention\n",
    "        metrics = network(data) #metrics = \n",
    "        # metrics = metrics.float()\n",
    "        target = target.float().cuda()\n",
    "        \n",
    "    # print(metrics)    ###metric's shape = [tps, latency]\n",
    "    # print(metrics.shape)   \n",
    "    # print(target)     ###target's shape = [tps, latency]\n",
    "    # print(target.shape)\n",
    "    \n",
    "        loss = criterion(metrics, target)\n",
    "        #mse = mean_squared_error(target.detach().cpu().numpy(), metrics.detach().cpu().numpy())\n",
    "        # mse = mse.cpu().numpy()\n",
    "        #loss = np.sqrt(mse)\n",
    "        \n",
    "       \n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        # update the model's weight(parameter value)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "        \n",
    "        # print(get_lr(optimizer))\n",
    "        \n",
    "        # total_acc = accuracy_score(target,metrics)\n",
    "        # total_acc = total_acc.cpu().numpy()\n",
    "        # correct_pred = metrics == target\n",
    "        # accuracy = correct_pred.float().mean()\n",
    "        # total_acc += accuracy \n",
    "    ## calculte mean of loss \n",
    "    total_loss /= len(dataloader_tr)\n",
    "    # total_acc /= len(dataloader_tr)\n",
    "    # print(f'[{epoch+1}epoch] tr_loss:{total_loss}')\n",
    "    \n",
    "    ## check a log (real-time experiment performance)\n",
    "    if epoch % 10 == 0:\n",
    "        total_val_loss = 0\n",
    "        # total_val_acc = 0\n",
    "        \n",
    "        ## validation performance check\n",
    "        network.eval()\n",
    "        with torch.no_grad():\n",
    "            # trues = []\n",
    "            # preds = []\n",
    "            preds = torch.Tensor().cuda()\n",
    "            trues = torch.Tensor().cuda()\n",
    "    \n",
    "            for data, target in dataloader_te:\n",
    "                data = np.array(data)\n",
    "                # onehot = clf.predict(data)\n",
    "                # stack_onehot = []\n",
    "                \n",
    "                # for i in onehot:\n",
    "                #     onehot_li = [0,0,0]\n",
    "                #     onehot_li[i] += 1 \n",
    "                #     stack_onehot.append(onehot_li)\n",
    "                # stack_onehot = torch.tensor(stack_onehot)\n",
    "\n",
    "                # cluster = stack_onehot.float().cuda()                \n",
    "\n",
    "\n",
    "                data = torch.tensor(data).float().cuda()\n",
    "                metrics = network(data)\n",
    "                #metrics = linear-attention-linear\n",
    "                metrics = metrics.float().cuda()\n",
    "             \n",
    "                \n",
    "                target = target.float().cuda()\n",
    "                # mse = mean_squared_error(target, metrics)\n",
    "                # loss = np.sqrt(mse)\n",
    "                loss = criterion(metrics, target)  \n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # correct_pred = metrics == target\n",
    "                # accuracy = correct_pred.float().mean()\n",
    "                # total_val_acc += accuracy\n",
    "                # total_val_acc = accuracy_score(target,metrics)\n",
    "                #r2_score를 위한\n",
    "                true = torch.Tensor(target).cuda()\n",
    "                preds = torch.cat((preds, metrics))\n",
    "                trues = torch.cat((trues, true))\n",
    "                    \n",
    "            r2_res = r2_score(trues.cpu().numpy().squeeze(), preds.cpu().numpy().squeeze(), multioutput='raw_values')\n",
    "            total_val_loss /= len(dataloader_te) \n",
    "            # total_val_acc /= len(dataloader_te)\n",
    "            \n",
    "            print(f'[{epoch+1}epoch] tr_loss:{total_loss:.4f} | val_loss:{total_val_loss:.4f} | R2_Score:{r2_res}') \n",
    "        \n",
    "\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4fb236bcdef68d7c49635ec867687876947f0f79362a1ae7a6ed0fd32d1a93c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
